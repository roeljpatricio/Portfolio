{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b70aac",
   "metadata": {},
   "source": [
    "## Collaborative Filters Via Gaussian Mixtures\n",
    "\n",
    "Our goal will be to complete entries in a matrix. The dataset used will be a sample of netflix users' ratings. Logically, each user only rates a minimal portion of the full content, resulting in a sparse matrix where most of its entries are not defined which we shall treat as 0s.\n",
    "\n",
    "To predict user ratings, we shall use gaussian mixtures models (GMMs), and to find such GMMs, the EM algorithm.\n",
    "\n",
    "### Expectation Maximization Algorithm\n",
    "\n",
    "The EM algorithm solves for GMMs where a sample is assumed to be generated by several normal distributions.\n",
    "For this particular case, we shall assume spherical normal distributions (the covariance matrix is diagonal). Thus being generated from  $\\mathcal{N}\\left(\\mathbf x^{(i)}; \\mu ^{(j)},\\sigma _ j^2 I\\right)$\n",
    "\n",
    "The EM algorithm works by maximizing the log likelihood of the mixture, with respect to the parameter set $\\theta = \\left\\{ p_1, \\dots , p_ K, \\mu ^{(1)}, \\dots , \\mu ^{(K)}, \\sigma _1^2, \\dots , \\sigma _ K^2\\right\\}$. There is no closed form solution in the regime of GMM, and therefore EM is an iterative way to find a local optimal $\\theta$.\n",
    "\n",
    "\n",
    "#### E-Step\n",
    "\n",
    "There are 2 steps in the algorithm. The first one involves calculating the posterior probability that point $\\mathbf x^{(i)}$ was generated by cluster, for every $i = 1,...,n$ and $j=1,...,K$. It is assumed that $\\theta$ is known.\n",
    "The posterior is defined as below:\n",
    "\n",
    "$$\\displaystyle  p(\\text {point }\\mathbf x^{(i)}\\text { was generated by cluster }j | \\mathbf x^{(i)}, \\theta ) \\triangleq p(j \\mid i) = \\frac{p_ j \\mathcal{N}\\left(\\mathbf x^{(i)}; \\mu ^{(j)},\\sigma _ j^2 I\\right)}{p(\\mathbf x^{(i)} \\mid \\theta )}$$.\n",
    "\n",
    "The result is that every point gets a soft count (a probability) for belonging to each cluster. This contrasts with K-means where each point is assigned to a unique cluster.\n",
    "\n",
    "#### M-Step\n",
    "\n",
    "The second step involves maximizing a proxy function (as defined below) over $\\theta$\n",
    "\n",
    "$$\\displaystyle  \\hat{\\ell }(\\mathbf x^{(1)},\\dots ,\\mathbf x^{(n)} \\mid \\theta ) \\triangleq \\sum _{i=1}^{n} \\sum _{j = 1}^ K p(j \\mid i) \\log \\left( \\frac{p\\left( \\mathbf x^{(i)} \\text { and } \\mathbf x^{(i)} \\text { generated by cluster }j \\mid \\theta \\right)}{p(j \\mid i)} \\right).$$\n",
    "\n",
    "Taking derivatives w.r.t. to each variable, setting them to 0 and solving we find that the optimizers are\n",
    "\n",
    "$$\\displaystyle  \\widehat{\\mu ^{(j)}} = \\frac{\\sum _{i = 1}^ n p (j \\mid i) \\mathbf x^{(i)}}{\\sum _{i=1}^ n p(j \\mid i)}$$\n",
    "\n",
    "$$\\displaystyle \\widehat{p_ j} = \\frac{1}{n}\\sum _{i = 1}^ n p(j \\mid i),$$\n",
    "\n",
    "$$\\displaystyle \\widehat{\\sigma _ j^2} = \\frac{\\sum _{i = 1}^ n p(j \\mid i) \\|  \\mathbf x^{(i)} - \\widehat{\\mu ^{(j)}} \\| ^2}{d \\sum _{i = 1}^ n p(j \\mid i)}.$$\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "Initialization  can be done either randomly or by finding the cluster centers through K-means and using the global variance of the dataset as the variance of each cluster. In such case, the proportion counts assigned by K-means can be used as the mixtures' weights.\n",
    "\n",
    "#### Missing Entries\n",
    "\n",
    "Finally, because we have missing values, we must focus on the marginal distribution of the observed values alone. Since we are under the regime of spherical, independent normal distributions, the mixture density can be written as:\n",
    "\n",
    "$$P(x_{C_ u}^{(u)} | \\theta ) = \\sum ^{K}_{j=1} \\pi _ j N(x_{C_ u}^{(u)}; \\mu _{C_ u}^{(j)}, \\sigma _ j^{2}I_{|C_ u| \\times |C_ u|})$$\n",
    "\n",
    "And we shall focus on the log domain for the E-step\n",
    "\n",
    "$$\\displaystyle  \\ell (j|u) = f(u,j)-\\log \\left(\\sum _{j=1}^{K}\\exp (f(u,j))\\right)$$\n",
    "\n",
    "Where $f(u,i)=\\log (\\pi _{i})+\\log \\left(N(x_{C_{u}}^{(u)};\\mu _{C_{u}}^{(i)},\\sigma _{i}^{2}I_{C_{u}\\times C_{u}})\\right)$, $x_{C_{u}}^{(u)}=\\{ x_{i}^{(u)}:\\,  i\\in C_{u}\\}$ is the set of observed values,  $\\mu _{C_ u}^{(j)}$ is the equivalent for the mean, and $I_{|C_ u| \\times |C_ u|})$ is the identity matrix in $|C_ u|$ dimensions.\n",
    "\n",
    "Optimizing for our parameters of interest we end up with:\n",
    "$$\\displaystyle  \\widehat{\\mu _ l^{(k)}} = \\frac{\\sum _{u = 1}^ n p(k \\mid u) \\delta (l,C_ u) x_ l^{(u)}}{\\sum _{u=1}^ n p(k \\mid u) \\delta (l,C_ u)},$$\n",
    "\n",
    "Where $\\delta (i,C_ u) = 1$ if $i \\in C_ u$ and $\\delta (i,C_ u) = 0$ if $i \\notin C_ u$.\n",
    "\n",
    "$$\\displaystyle  \\widehat{\\sigma _ k^2} = \\frac{1}{\\sum _{u=1}^ n |C_ u| p(k \\mid u)} \\sum _{u = 1}^ n p(k \\mid u) \\| x_{C_ u}^{(u)} - \\widehat{\\mu _{C_ u}^{(k)}}\\| ^2,$$\n",
    "\n",
    "$$\\displaystyle \\widehat{\\pi _ k} = \\frac{1}{n} \\sum _{u=1}^ n p(k \\mid u).$$\n",
    "\n",
    "(We let go of the proof in this step for conciseness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6b0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from common import *  # Functions in common were provided by assignment statement! - except for bic\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bb3abd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"E-step: Softly assigns each datapoint to a gaussian component\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        mixture: the current gaussian mixture\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the assignment\n",
    "\n",
    "    \"\"\"\n",
    "    n , K = X.shape[0] , mixture.var.shape[0]\n",
    "    post = np.zeros((n,K))\n",
    "    LL=0    # Log-likelihood\n",
    "    for i in range (n):\n",
    "        mask = X[i] != 0  # Mask to select observed values only\n",
    "                     \n",
    "        mu , var , P = mixture.mu[:,mask] , np.expand_dims(mixture.var,1) , np.expand_dims(mixture.p,1) \n",
    "        d = mu.shape[1]\n",
    "        \n",
    "        XX = np.tile(X[i][mask], (K, 1))  # np.tile is used to allow for calculations in matrix form\n",
    "        den = (-d/2)*np.log(2*np.pi*var)\n",
    "        exp = -np.linalg.norm(XX-mu,axis=1,keepdims=2)**2/(2*var)\n",
    "        l_norm = den+exp\n",
    "        f = np.log(P+1e-16) + l_norm  # 1e-16 added for numerical stability, f as f(u,i)\n",
    "        LL += logsumexp(f)\n",
    "        post[i,:]= np.exp((f - np.tile(logsumexp(f),(K,1))).T)\n",
    "        \n",
    "\n",
    "    return (post,LL)\n",
    "\n",
    "\n",
    "def mstep(X: np.ndarray, post: np.ndarray, mixture: GaussianMixture,\n",
    "          min_variance: float = .25) -> GaussianMixture:\n",
    "    \"\"\"M-step: Updates the gaussian mixture by maximizing the log-likelihood\n",
    "    of the weighted dataset\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        mixture: the current gaussian mixture\n",
    "        min_variance: the minimum variance for each gaussian\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "    \"\"\"\n",
    "    n , K = X.shape[0] , post.shape[1]\n",
    "    mu , var , P = mixture.mu , mixture.var , mixture.p\n",
    "    \n",
    "    N = post.sum(axis = 0)\n",
    "    P = N/n\n",
    "\n",
    "    for j in range (K):\n",
    "        d = X.shape[1]\n",
    "        ind = X.copy()\n",
    "        ind[ind!=0] = 1.0  # indicator function\n",
    "        \n",
    "        mu_hat = np.sum(post[:,[j]]*X,axis = 0)/np.sum(ind*post[:,[j]],axis = 0)\n",
    "        mask = np.sum(post[:,[j]]*ind,axis=0) >= 1  # Mask to update mu when an entire point is assigned to a cluster\n",
    "        mu_hat_j = mu[j].copy()\n",
    "        mu_hat_j[mask] = mu_hat[mask]\n",
    "        mu[j] = mu_hat_j\n",
    "        \n",
    "        d_Cu = np.sum(ind, axis = 1,keepdims=1)  # Dimension of set of observed points\n",
    "        denom = np.sum(d_Cu*post[:,[j]])  # Denominator for variance update\n",
    "        mutile=np.tile(mu[j],(n,1))*ind\n",
    "        sq_dist = np.linalg.norm(X - mutile, axis = 1,keepdims = 1)**2\n",
    "        var[j] = (post[:,[j]]*sq_dist).sum(axis= 0)/denom\n",
    "    var [ var < min_variance ] = min_variance\n",
    "    \n",
    "    return (GaussianMixture(mu, var, P))\n",
    "    \n",
    "\n",
    "def fill_matrix(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:\n",
    "    \"\"\"Fills an incomplete matrix according to a mixture model\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array of incomplete data (incomplete entries =0)\n",
    "        mixture: a mixture of gaussians\n",
    "\n",
    "    Returns\n",
    "        np.ndarray: a (n, d) array with completed data\n",
    "    \"\"\"\n",
    "\n",
    "    XX = X.copy()    \n",
    "    post = estep(X,mixture)[0]\n",
    "    mask = X == 0\n",
    "    \n",
    "    E_matrix = post @ mixture.mu  # Matrix with expected values for all points\n",
    "    XX[mask] = E_matrix[mask]  # Update where there are missing values\n",
    "    return XX\n",
    "\n",
    "def run(X: np.ndarray, mixture: GaussianMixture,\n",
    "            post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:\n",
    "        \"\"\"Runs the mixture model\n",
    "    \n",
    "        Args:\n",
    "            X: (n, d) array holding the data\n",
    "            post: (n, K) array holding the soft counts\n",
    "                for all components for all examples\n",
    "    \n",
    "        Returns:\n",
    "            GaussianMixture: the new gaussian mixture\n",
    "            np.ndarray: (n, K) array holding the soft counts\n",
    "                for all components for all examples\n",
    "            float: log-likelihood of the current assignment\n",
    "        \"\"\"\n",
    "        old = None\n",
    "        new = None\n",
    "        while old is None or new - old > abs(new)*1e-6:\n",
    "            \n",
    "            old=new\n",
    "            post , new = estep(X,mixture)\n",
    "            mixture = mstep(X,post,mixture)         \n",
    "\n",
    "        return (mixture, post, new)\n",
    "    \n",
    "def super_run(X,K,n_seeds):\n",
    "        \"\"\"Executes several runs with different seed values\n",
    "    \n",
    "        Args:\n",
    "            X: (n, d) array holding the data\n",
    "            K: int, number of clusters\n",
    "            n_seeds: integer, used as range in for loop\n",
    "    \n",
    "        Returns:\n",
    "            GaussianMixture: best performing GaussianMixture\n",
    "            LL: float, log likelihood with best GaussianMixture\n",
    "        \"\"\"\n",
    "        LL = []\n",
    "        GM = []\n",
    "        for seed in range (n_seeds):\n",
    "\n",
    "            X_init = init(X,K,seed)\n",
    "            run_result = run(X,X_init[0],X_init[1]) \n",
    "            GM += [run_result[0]]\n",
    "            LL += [run_result[2]] #[seed]\n",
    "            print(f'LL for K = {K} and seed = {seed}, \\n{LL[seed]}')\n",
    "        maxLL = max(LL) ; best_GM = GM[LL.index(maxLL)]\n",
    "        return best_GM,maxLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae496ef8",
   "metadata": {},
   "source": [
    "With our algorithm ready to calculate the optimal $\\theta$ we can now proceed to create the rest of the functions we need to predict values.\n",
    "Fill matrix will replace missing values (0s) with expected values.\n",
    "Run makes a full run of the algorithm, returning a gaussian mixture, the soft counts and the log likelihood. Super run iterates over a user defined number of seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_matrix(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:\n",
    "    \"\"\"Fills an incomplete matrix according to a mixture model\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array of incomplete data (incomplete entries =0)\n",
    "        mixture: a mixture of gaussians\n",
    "\n",
    "    Returns\n",
    "        np.ndarray: a (n, d) array with completed data\n",
    "    \"\"\"\n",
    "\n",
    "    XX = X.copy()    \n",
    "    post = estep(X,mixture)[0]\n",
    "    mask = X == 0\n",
    "    \n",
    "    E_matrix = post @ mixture.mu  # Matrix with expected values for all points\n",
    "    XX[mask] = E_matrix[mask]  # Update where there are missing values\n",
    "    return XX\n",
    "\n",
    "def run(X: np.ndarray, mixture: GaussianMixture,\n",
    "            post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:\n",
    "        \"\"\"Runs the mixture model\n",
    "    \n",
    "        Args:\n",
    "            X: (n, d) array holding the data\n",
    "            post: (n, K) array holding the soft counts\n",
    "                for all components for all examples\n",
    "    \n",
    "        Returns:\n",
    "            GaussianMixture: the new gaussian mixture\n",
    "            np.ndarray: (n, K) array holding the soft counts\n",
    "                for all components for all examples\n",
    "            float: log-likelihood of the current assignment\n",
    "        \"\"\"\n",
    "        old = None\n",
    "        new = None\n",
    "        while old is None or new - old > abs(new)*1e-6:\n",
    "            \n",
    "            old=new\n",
    "            post , new = estep(X,mixture)\n",
    "            mixture = mstep(X,post,mixture)         \n",
    "\n",
    "        return (mixture, post, new)\n",
    "    \n",
    "def super_run(X,K,n_seeds):\n",
    "        \"\"\"Executes several runs with different seed values\n",
    "    \n",
    "        Args:\n",
    "            X: (n, d) array holding the data\n",
    "            K: int, number of clusters\n",
    "            n_seeds: integer, used as range in for loop\n",
    "    \n",
    "        Returns:\n",
    "            GaussianMixture: best performing GaussianMixture\n",
    "            LL: float, log likelihood with best GaussianMixture\n",
    "        \"\"\"\n",
    "        LL = []\n",
    "        GM = []\n",
    "        for seed in range (n_seeds):\n",
    "\n",
    "            X_init = init(X,K,seed)\n",
    "            run_result = run(X,X_init[0],X_init[1]) \n",
    "            GM += [run_result[0]]\n",
    "            LL += [run_result[2]] #[seed]\n",
    "            print(f'LL for K = {K} and seed = {seed}, \\n{LL[seed]}')\n",
    "        maxLL = max(LL) ; best_GM = GM[LL.index(maxLL)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0ff3c",
   "metadata": {},
   "source": [
    "We proceed to load the dataset and calculate the best performing Gaussian Mixture. We shall use 12 clusters and 5 seeds.\n",
    "(Note that we know that this is the optimal solution, We would iterate over several cluster numbers but it would be too long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cec80cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-166-02c00728c08f>:61: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mu_hat = np.sum(post[:,[j]]*X,axis = 0)/np.sum(ind*post[:,[j]],axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LL for K = 12 and seed = 0, \n",
      "-1399803.0466569131\n",
      "LL for K = 12 and seed = 1, \n",
      "-1390234.4223469417\n",
      "LL for K = 12 and seed = 2, \n",
      "-1416862.401151279\n",
      "LL for K = 12 and seed = 3, \n",
      "-1393521.3929897777\n",
      "LL for K = 12 and seed = 4, \n",
      "-1416733.8083763549\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt('netflix_incomplete.txt')\n",
    "best_GM=super_run(X,12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1f9d2bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE between predicted and true values is 0.480491\n"
     ]
    }
   ],
   "source": [
    "X_pred = fill_matrix(W,Best_GM[0]);\n",
    "X_gold = np.loadtxt('netflix_complete.txt')\n",
    "print( f'The RMSE between predicted and true values is {np.round(rmse(X_pred,X_gold),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e69f7",
   "metadata": {},
   "source": [
    "For assessment purposes, test incomplete, test complete and test solutions can be used to verify that the algorithm works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282340c",
   "metadata": {},
   "source": [
    "#### And that's all!\n",
    "### Thank you for reading this far, I hope you've  enjoyed the exercise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
